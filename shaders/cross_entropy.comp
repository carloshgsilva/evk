#include "evk.frag"

#extension GL_EXT_shader_explicit_arithmetic_types_float16 : enable
#extension GL_EXT_shader_16bit_storage : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_EXT_control_flow_attributes : enable

layout(buffer_reference) buffer BufferFp16 { float16_t x[]; };
layout(buffer_reference) buffer BufferU16 { uint16_t x[]; };
layout(buffer_reference) buffer BufferFp32 { float x[]; };

PUSH(
    BufferFp16 logitsBuffer;   // (totalPositions, vocabSize)
    BufferU16 targetBuffer;    // (totalPositions)
    BufferFp16 gradBuffer;     // (totalPositions, vocabSize)
    BufferFp32 accumBuffer;    // float[2] -> {totalLoss, validCount}
    uint vocabSize;            // V
    uint totalPositions;       // B * N
)

// Shared reduction buffers sized for up to 4 subgroups (local_size_x = 128)
shared float sharedMax[4];
shared float sharedDenom[4];
shared float sharedTarget[4];

// Workgroup processes one row; multiple subgroups cooperate for less loop work
COMPUTE(128, 1, 1)
void main() {
    uint row = gl_WorkGroupID.x;
    if (row >= totalPositions) return;

    uint lid = gl_LocalInvocationID.x; // 0..127
    uint subgroupId = gl_SubgroupID;
    uint subgroupCount = gl_NumSubgroups;
    uint subgroupLane = gl_SubgroupInvocationID;
    uint rowStart = row * vocabSize;
    uint targetIdx = uint(targetBuffer.x[row]);
    bool isValidTarget = (targetIdx != 0u) && (targetIdx < vocabSize);

    // Skip expensive softmax work for ignored positions; just zero gradients.
    if (!isValidTarget) {
        for (uint j = lid; j < vocabSize; j += 128) {
            gradBuffer.x[rowStart + j] = float16_t(0.0);
        }
        return;
    }

    // 1) Row-wise max for stability
    float localMax = -3.402823466e+38; // -FLT_MAX
    for (uint j = lid; j < vocabSize; j += 128) {
        float v = float(logitsBuffer.x[rowStart + j]);
        v = clamp(v, -65000.0, 65000.0);
        localMax = max(localMax, v);
    }
    float subgroupMaxVal = subgroupMax(localMax);
    if (subgroupLane == 0) {
        sharedMax[subgroupId] = subgroupMaxVal;
    }
    barrier();
    if (subgroupId == 0 && subgroupLane == 0) {
        float rowMaxVal = sharedMax[0];
        for (uint s = 1; s < subgroupCount && s < 4; ++s) {
            rowMaxVal = max(rowMaxVal, sharedMax[s]);
        }
        sharedMax[0] = rowMaxVal;
    }
    barrier();
    float rowMax = sharedMax[0];

    // 2) Sum of exp
    float localSum = 0.0;
    for (uint j = lid; j < vocabSize; j += 128) {
        float v = float(logitsBuffer.x[rowStart + j]);
        v = clamp(v, -65000.0, 65000.0);
        localSum += exp(v - rowMax);
    }
    float subgroupDenom = subgroupAdd(localSum);
    if (subgroupLane == 0) {
        sharedDenom[subgroupId] = subgroupDenom;
    }
    barrier();
    if (subgroupId == 0 && subgroupLane == 0) {
        float denom = 0.0;
        for (uint s = 0; s < subgroupCount && s < 4; ++s) {
            denom += sharedDenom[s];
        }
        sharedDenom[0] = denom;
    }
    barrier();
    float safeDenom = max(sharedDenom[0], 1e-20);
    float invDenom = 1.0 / safeDenom;
    float logDenom = log(safeDenom);

    // 3) Gradients + target log prob
    float targetLogProb = 0.0;
    for (uint j = lid; j < vocabSize; j += 128) {
        float logit = float(logitsBuffer.x[rowStart + j]);
        logit = clamp(logit, -65000.0, 65000.0);
        float softmax_val = exp(logit - rowMax) * invDenom;

        float grad = isValidTarget ? softmax_val : 0.0;
        if (isValidTarget && j == targetIdx) {
            grad -= 1.0;
            targetLogProb = logit - rowMax - logDenom;
        }
        gradBuffer.x[rowStart + j] = float16_t(grad);
    }

    // Reduce target contribution across subgroups
    float subgroupTarget = subgroupAdd(targetLogProb);
    if (subgroupLane == 0) {
        sharedTarget[subgroupId] = subgroupTarget;
    }
    barrier();
    if (subgroupId == 0 && subgroupLane == 0) {
        float totalTarget = 0.0;
        for (uint s = 0; s < subgroupCount && s < 4; ++s) {
            totalTarget += sharedTarget[s];
        }
        sharedTarget[0] = totalTarget;
    }
    barrier();

    // 4) Accumulate totals using atomics
    if (isValidTarget) {
        float reducedTarget = sharedTarget[0];
        if (lid == 0) {
            atomicAdd(accumBuffer.x[0], -reducedTarget); // total loss
            atomicAdd(accumBuffer.x[1], 1.0);            // valid count
        }
    }
}
