#include "evk.frag"

#extension GL_EXT_shader_explicit_arithmetic_types_float16 : enable
#extension GL_EXT_shader_16bit_storage : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_EXT_control_flow_attributes : enable

layout(buffer_reference) buffer BufferFp16 { float16_t x[]; };
layout(buffer_reference) buffer BufferU16 { uint16_t x[]; };
layout(buffer_reference) buffer BufferFp32 { float x[]; };

PUSH(
    BufferFp16 logitsBuffer;   // (totalPositions, vocabSize)
    BufferU16 targetBuffer;    // (totalPositions)
    BufferFp16 gradBuffer;     // (totalPositions, vocabSize)
    BufferFp32 accumBuffer;    // float[2] -> {totalLoss, validCount}
    uint vocabSize;            // V
    uint totalPositions;       // B * N
)

// Shared reduction buffers sized for typical subgroup counts (covers up to 64 subgroups)
const uint MAX_SUBGROUPS = 64;
shared float sharedMax[MAX_SUBGROUPS];
shared float sharedDenom[MAX_SUBGROUPS];
shared float sharedTarget[MAX_SUBGROUPS];
shared float sharedInvValid;

const float LOG2E = 1.4426950408889634;
const float LN2 = 0.6931471805599453;

// Workgroup processes one row; multiple subgroups cooperate for less loop work
COMPUTE(256, 1, 1)
void main() {
    uint row = gl_WorkGroupID.x;
    if (row >= totalPositions) return;

    uint lid = gl_LocalInvocationID.x; // 0..127
    uint subgroupId = gl_SubgroupID;
    uint subgroupCount = gl_NumSubgroups;
    uint subgroupLane = gl_SubgroupInvocationID;
    uint rowStart = row * vocabSize;
    uint targetIdx = uint(targetBuffer.x[row]);
    bool isValidTarget = (targetIdx != 0u) && (targetIdx < vocabSize);

    // Skip expensive softmax work for ignored positions; just zero gradients.
    if (!isValidTarget) {
        for (uint j = lid; j < vocabSize; j += gl_WorkGroupSize.x) {
            gradBuffer.x[rowStart + j] = float16_t(0.0);
        }
        return;
    }

    // Broadcast global scale factor (1 / validCount) for this dispatch
    if (lid == 0) {
        float count = accumBuffer.x[1];
        sharedInvValid = (count > 0.0) ? (1.0 / count) : 0.0;
    }
    barrier();
    float invValid = sharedInvValid;

    // 1) Row-wise max for stability
    float localMax = -3.402823466e+38; // -FLT_MAX
    for (uint j = lid; j < vocabSize; j += gl_WorkGroupSize.x) {
        float v = float(logitsBuffer.x[rowStart + j]);
        v = clamp(v, -65000.0, 65000.0);
        localMax = max(localMax, v);
    }
    float subgroupMaxVal = subgroupMax(localMax);
    if (subgroupLane == 0) {
        sharedMax[subgroupId] = subgroupMaxVal;
    }
    barrier();
    if (subgroupId == 0 && subgroupLane == 0) {
        float rowMaxVal = sharedMax[0];
        uint subgroupLimit = min(subgroupCount, MAX_SUBGROUPS);
        for (uint s = 1; s < subgroupLimit; ++s) {
            rowMaxVal = max(rowMaxVal, sharedMax[s]);
        }
        sharedMax[0] = rowMaxVal;
    }
    barrier();
    float rowMax = sharedMax[0];

    // 2) Sum of exp
    float localSum = 0.0;
    for (uint j = lid; j < vocabSize; j += gl_WorkGroupSize.x) {
        float logit = float(logitsBuffer.x[rowStart + j]);
        logit = clamp(logit, -65000.0, 65000.0);
        localSum += exp2((logit - rowMax) * LOG2E);
    }
    float subgroupDenom = subgroupAdd(localSum);
    if (subgroupLane == 0) {
        sharedDenom[subgroupId] = subgroupDenom;
    }
    barrier();
    if (subgroupId == 0 && subgroupLane == 0) {
        float denom = 0.0;
        uint subgroupLimit = min(subgroupCount, MAX_SUBGROUPS);
        for (uint s = 0; s < subgroupLimit; ++s) {
            denom += sharedDenom[s];
        }
        sharedDenom[0] = denom;
    }
    barrier();
    float safeDenom = max(sharedDenom[0], 1e-20);
    float invDenom = 1.0 / safeDenom;
    float logDenom = log2(safeDenom) * LN2;
    float targetLogProb = 0.0;

    // 3) Gradients
    for (uint j = lid; j < vocabSize; j += gl_WorkGroupSize.x) {
        float logit = float(logitsBuffer.x[rowStart + j]);
        logit = clamp(logit, -65000.0, 65000.0);
        float softmax_val = exp2((logit - rowMax) * LOG2E) * invDenom;

        float grad = isValidTarget ? (softmax_val * invValid) : 0.0;
        if (isValidTarget && j == targetIdx) {
            grad -= invValid;
            targetLogProb = logit - rowMax - logDenom;
        }
        gradBuffer.x[rowStart + j] = float16_t(grad);
    }

    // Reduce target contribution across subgroups
    float subgroupTarget = subgroupAdd(targetLogProb);
    if (subgroupLane == 0) {
        sharedTarget[subgroupId] = subgroupTarget;
    }
    barrier();
    if (subgroupId == 0 && subgroupLane == 0) {
        float totalTarget = 0.0;
        uint subgroupLimit = min(subgroupCount, MAX_SUBGROUPS);
        for (uint s = 0; s < subgroupLimit; ++s) {
            totalTarget += sharedTarget[s];
        }
        sharedTarget[0] = totalTarget;
    }
    barrier();

    // 4) Accumulate totals using atomics
    if (isValidTarget && lid == 0) {
        atomicAdd(accumBuffer.x[0], -sharedTarget[0]); // total loss (unscaled)
    }
}
