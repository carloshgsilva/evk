#include "evk.frag"

CONSTANT(0) uint TRANSPOSE_A = 0;
CONSTANT(1) uint SUM_C = 0;

PUSH(
    int aBufferRID;
    int bBufferRID;
    int cBufferRID;
    uint B; // B = batch size (optional, set to 1 if unused)
    uint M; // M = rows of A / output rows
    uint K; // K = shared dimension (columns of A / rows of B)
    uint N; // N = columns of B / output columns
)

BINDING_BUFFER_RW(float_buffer,
    float at[];
)

#if 0 // tiled manual matmul

// TILED MATMUL
// Expects buffers packed in row-major:
// A: B x M x K at binding aBufferRID
// B: B x K x N at binding bBufferRID
// C: B x M x N at binding cBufferRID

// Tune tile size for target GPU. Keep local_size_x * local_size_y <= 1024 generally.
#define TILE 16
// shared memory tiles must be declared at global scope
shared float Asub[TILE][TILE];
shared float Bsub[TILE][TILE];

COMPUTE(TILE, TILE, 1)
void main() {
    // global indices
    uvec2 gid = uvec2(gl_GlobalInvocationID.xy);
    uint row = gid.y; // y is row (M)
    uint col = gid.x; // x is col (N)
    // batch index (z dimension)
    uint batchIdx = gl_GlobalInvocationID.z;
    // compute per-batch base offsets (assumes host arranges buffers per-batch contiguously)
    uint aBase = batchIdx * (M * K);
    uint bBase = batchIdx * (K * N);
    uint cBase = batchIdx * (M * N);

    // Shared memory tiles for A and B (declared at global scope)
    float acc = 0.0;

    // Loop over tiles of K dimension
    for (uint t = 0; t < (K + TILE - 1) / TILE; ++t) {
        // Each invocation loads one element of A and one of B into shared memory if within bounds
        uint aRow = row;
        uint aCol = t * TILE + gl_LocalInvocationID.x;
        uint bRow = t * TILE + gl_LocalInvocationID.y;
        uint bCol = col;

        // compute flat indices for buffers: A is M x K row-major, B is K x N row-major
        float aVal = 0.0;
        float bVal = 0.0;
        if (aRow < M && aCol < K) {
            // compute index relative to per-batch A base
            // respect TRANSPOSE_A specialization constant: when set, A is stored transposed (K x M)
            uint aIndex;
            if (TRANSPOSE_A == 0u) {
                aIndex = aBase + aRow * K + aCol;
            } else {
                // stored as K x M row-major when transposed: index = aBase + col * M + row
                aIndex = aBase + aCol * M + aRow;
            }
            aVal = float_buffer[aBufferRID].at[aIndex];
        }
        if (bRow < K && bCol < N) {
            // compute index relative to per-batch B base
            uint bIndex = bBase + bRow * N + bCol;
            bVal = float_buffer[bBufferRID].at[bIndex];
        }

        Asub[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = aVal;
        Bsub[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = bVal;

        // ensure tile loads complete
        barrier();

        // compute partial products
        for (uint k = 0; k < TILE; ++k) {
            uint globalK = t * TILE + k;
            if (globalK < K) {
                acc += Asub[gl_LocalInvocationID.y][k] * Bsub[k][gl_LocalInvocationID.x];
            }
        }

        // wait for all threads before next tile
        barrier();
    }

    // Write output if within bounds
    if (row < M && col < N) {
        // write result to per-batch C base
        uint outIndex = cBase + row * N + col;
        if (SUM_C == 0u) {
            float_buffer[cBufferRID].at[outIndex] = acc;
        } else {
            float_buffer[cBufferRID].at[outIndex] += acc;
        }
    }
}

#else // coop matmul

#extension GL_EXT_shader_explicit_arithmetic_types_float16 : enable
#extension GL_EXT_shader_16bit_storage : enable
#extension GL_KHR_cooperative_matrix : enable
#extension GL_KHR_memory_scope_semantics : enable

// We build a 32x32 macro tile using four 16x16 cooperative subtiles.
#define SUB_TILE 16
#define MACRO_TILE 32

// Shared memory staging for a macro 32x32 tile per K-slice (16)
// Asmem: 32x16 (rows along M, cols along K-slice)
// Bsmem: 16x32 (rows along K-slice, cols along N)
shared float16_t Asmem[MACRO_TILE * SUB_TILE];
shared float16_t Bsmem[SUB_TILE * MACRO_TILE];

// 4 subgroups per workgroup (y=0..3), each subgroup is 32 lanes in X
COMPUTE(32, 4, 1)
void main() {
    // Workgroup mapping (with host dispatch: Y = tilesCols*tilesRows, Z = batch):
    //   X: fixed to 1 workgroup; local_size_x=32 satisfies subgroup constraint
    //   Y: linearized 2D tile index over (rows, cols)
    //   Z: batch index
    uint tilesRows = (M + MACRO_TILE - 1u) / MACRO_TILE;
    uint tilesCols = (N + MACRO_TILE - 1u) / MACRO_TILE;
    uint batchIdx = gl_WorkGroupID.z;
    uint tileLinear = gl_WorkGroupID.y;
    uint tileRowIndex = (tilesCols == 0u) ? 0u : (tileLinear / tilesCols);
    uint tileColIndex = (tilesCols == 0u) ? 0u : (tileLinear % tilesCols);

    uint aBase = batchIdx * (M * K);
    uint bBase = batchIdx * (K * N);
    uint cBase = batchIdx * (M * N);

    // Macro tile origin for this workgroup
    uint tileRow0 = tileRowIndex * MACRO_TILE;
    uint tileCol0 = tileColIndex * MACRO_TILE;

    // Map the 4 subgroups (y=0..3) to 2x2 subtiles inside the 32x32 tile
    uint subGroupId = gl_LocalInvocationID.y; // 0..3, one subgroup per y-row
    uint subRow = subGroupId & 1u;            // 0 or 1 (rows of sub-tiles)
    uint subCol = subGroupId >> 1;            // 0 or 1 (cols of sub-tiles)
    uint subRowOffset = subRow * SUB_TILE;    // 0 or 16
    uint subColOffset = subCol * SUB_TILE;    // 0 or 16

    // Cooperative matrices per subgroup (each handles 16x16)
    coopmat<float16_t, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseA> tile_A;
    coopmat<float16_t, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseB> tile_B;
    coopmat<float, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseAccumulator> accumulator =
        coopmat<float, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseAccumulator>(0.0);

    // Loop K in 16-wide tiles; stage macro tiles to shared memory then load coopmats from shared
    for (uint t = 0; t < (K + SUB_TILE - 1u) / SUB_TILE; ++t) {
        // Cooperative load into shared memory across the whole workgroup
        const uint wgSize = gl_WorkGroupSize.x * gl_WorkGroupSize.y * gl_WorkGroupSize.z;
        uint linearThread = gl_LocalInvocationID.z * (gl_WorkGroupSize.x * gl_WorkGroupSize.y)
                          + gl_LocalInvocationID.y * gl_WorkGroupSize.x
                          + gl_LocalInvocationID.x;

        const uint totalA = MACRO_TILE * SUB_TILE; // 32*16
        const uint totalB = SUB_TILE * MACRO_TILE; // 16*32
        const uint total  = totalA + totalB;

        for (uint idx = linearThread; idx < total; idx += wgSize) {
            if (idx < totalA) {
                // Fill Asmem row-major [32 x 16]
                uint i = idx;
                uint aRow = i / SUB_TILE;      // 0..31
                uint aCol = i % SUB_TILE;      // 0..15
                uint gRowA = tileRow0 + aRow;
                uint gColA = t * SUB_TILE + aCol;
                float aVal = 0.0;
                if (gRowA < M && gColA < K) {
                    uint aIndex;
                    if (TRANSPOSE_A == 0u) {
                        aIndex = aBase + gRowA * K + gColA;
                    } else {
                        // A stored as KxM when transposed
                        aIndex = aBase + gColA * M + gRowA;
                    }
                    aVal = float_buffer[aBufferRID].at[aIndex];
                }
                Asmem[aRow * SUB_TILE + aCol] = float16_t(aVal);
            } else {
                // Fill Bsmem row-major [16 x 32]
                uint j = idx - totalA;
                uint bRow = j / MACRO_TILE;    // 0..15
                uint bCol = j % MACRO_TILE;    // 0..31
                uint gRowB = t * SUB_TILE + bRow;
                uint gColB = tileCol0 + bCol;
                float bVal = 0.0;
                if (gRowB < K && gColB < N) {
                    uint bIndex = bBase + gRowB * N + gColB;
                    bVal = float_buffer[bBufferRID].at[bIndex];
                }
                Bsmem[bRow * MACRO_TILE + bCol] = float16_t(bVal);
            }
        }

        // Ensure shared tiles are fully populated before coop loads
        barrier();

        // Compute subgroup-specific top-left offsets inside shared tiles
        uint aSharedOffset = subRowOffset * SUB_TILE; // (row * width)
        uint bSharedOffset = subColOffset;            // (row=0, col offset)

        // Load cooperative tiles from shared memory
        coopMatLoad(tile_A, Asmem, aSharedOffset, SUB_TILE, 0);
        coopMatLoad(tile_B, Bsmem, bSharedOffset, MACRO_TILE, 0);

        // FMA accumulate
        accumulator = coopMatMulAdd(tile_A, tile_B, accumulator);

        // Wait before next K-slice staging reuse
        barrier();
    }

    // Store the resulting 16x16 subtile into C. We assume M,N are multiples of 16.
    // For ragged edges, the host should pad or dispatch trimmed groups accordingly.
    uint cElement = cBase + (tileRow0 + subRowOffset) * N + (tileCol0 + subColOffset);
    uint cStride = N;
    if (SUM_C == 0u) {
        coopMatStore(accumulator, float_buffer[cBufferRID].at, cElement, cStride, 0);
    } else {
        coopmat<float, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseAccumulator> cTile;
        coopMatLoad(cTile, float_buffer[cBufferRID].at, cElement, cStride, 0);
        cTile = cTile + accumulator;
        coopMatStore(cTile, float_buffer[cBufferRID].at, cElement, cStride, 0);
    }
}
#endif