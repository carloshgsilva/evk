#include "evk.frag"

CONSTANT(0) uint TRANSPOSE_A = 0;
CONSTANT(1) uint SUM_C = 0;

PUSH(
    int aBufferRID;
    int bBufferRID;
    int cBufferRID;
    uint B; // B = batch size (optional, set to 1 if unused)
    uint M; // M = rows of A / output rows
    uint K; // K = shared dimension (columns of A / rows of B)
    uint N; // N = columns of B / output columns
)

BINDING_BUFFER_RW(float_buffer,
    float at[];
)

#if 0 // tiled manual matmul

// TILED MATMUL
// Expects buffers packed in row-major:
// A: B x M x K at binding aBufferRID
// B: B x K x N at binding bBufferRID
// C: B x M x N at binding cBufferRID

// Tune tile size for target GPU. Keep local_size_x * local_size_y <= 1024 generally.
#define TILE 16
// shared memory tiles must be declared at global scope
shared float Asub[TILE][TILE];
shared float Bsub[TILE][TILE];

COMPUTE(TILE, TILE, 1)
void main() {
    // global indices
    uvec2 gid = uvec2(gl_GlobalInvocationID.xy);
    uint row = gid.y; // y is row (M)
    uint col = gid.x; // x is col (N)
    // batch index (z dimension)
    uint batchIdx = gl_GlobalInvocationID.z;
    // compute per-batch base offsets (assumes host arranges buffers per-batch contiguously)
    uint aBase = batchIdx * (M * K);
    uint bBase = batchIdx * (K * N);
    uint cBase = batchIdx * (M * N);

    // Shared memory tiles for A and B (declared at global scope)
    float acc = 0.0;

    // Loop over tiles of K dimension
    for (uint t = 0; t < (K + TILE - 1) / TILE; ++t) {
        // Each invocation loads one element of A and one of B into shared memory if within bounds
        uint aRow = row;
        uint aCol = t * TILE + gl_LocalInvocationID.x;
        uint bRow = t * TILE + gl_LocalInvocationID.y;
        uint bCol = col;

        // compute flat indices for buffers: A is M x K row-major, B is K x N row-major
        float aVal = 0.0;
        float bVal = 0.0;
        if (aRow < M && aCol < K) {
            // compute index relative to per-batch A base
            // respect TRANSPOSE_A specialization constant: when set, A is stored transposed (K x M)
            uint aIndex;
            if (TRANSPOSE_A == 0u) {
                aIndex = aBase + aRow * K + aCol;
            } else {
                // stored as K x M row-major when transposed: index = aBase + col * M + row
                aIndex = aBase + aCol * M + aRow;
            }
            aVal = float_buffer[aBufferRID].at[aIndex];
        }
        if (bRow < K && bCol < N) {
            // compute index relative to per-batch B base
            uint bIndex = bBase + bRow * N + bCol;
            bVal = float_buffer[bBufferRID].at[bIndex];
        }

        Asub[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = aVal;
        Bsub[gl_LocalInvocationID.y][gl_LocalInvocationID.x] = bVal;

        // ensure tile loads complete
        barrier();

        // compute partial products
        for (uint k = 0; k < TILE; ++k) {
            uint globalK = t * TILE + k;
            if (globalK < K) {
                acc += Asub[gl_LocalInvocationID.y][k] * Bsub[k][gl_LocalInvocationID.x];
            }
        }

        // wait for all threads before next tile
        barrier();
    }

    // Write output if within bounds
    if (row < M && col < N) {
        // write result to per-batch C base
        uint outIndex = cBase + row * N + col;
        if (SUM_C == 0u) {
            float_buffer[cBufferRID].at[outIndex] = acc;
        } else {
            float_buffer[cBufferRID].at[outIndex] += acc;
        }
    }
}

#else // coop matmul

#extension GL_EXT_shader_explicit_arithmetic_types_float16 : enable
#extension GL_EXT_shader_16bit_storage : enable
#extension GL_KHR_cooperative_matrix : enable
#extension GL_KHR_memory_scope_semantics : enable

// Workgroup computes one 32x32 C tile using 4 subgroups (2x2 of 16x16 subtiles)
#define TILE 32
#define SUB_TILE 16

// Shared staging for A and B tiles in fp16
shared float16_t Asub[TILE * TILE];
shared float16_t Bsub[TILE * TILE];

COMPUTE(32, 4, 1)
void main() {
    // Workgroup mapping (host dispatch: X = tilesCols, Y = tilesRows, Z = batch)
    uint tilesRows = (M + TILE - 1u) / TILE;
    uint tilesCols = (N + TILE - 1u) / TILE;
    uint batchIdx = gl_WorkGroupID.z;
    uint tileRowIndex = gl_WorkGroupID.y;
    uint tileColIndex = gl_WorkGroupID.x;

    uint aBase = batchIdx * (M * K);
    uint bBase = batchIdx * (K * N);
    uint cBase = batchIdx * (M * N);

    // 32x32 tile origin for this workgroup
    uint tileRow0 = tileRowIndex * TILE;
    uint tileCol0 = tileColIndex * TILE;

    // Optional guard if host dispatch over-covers
    if (tileRow0 >= M || tileCol0 >= N) {
        return;
    }

    // Map 4 subgroups across Y dimension into a 2x2 grid of 16x16 subtiles
    // local_size_x = 32 (one subgroup), local_size_y = 4 (four subgroups total)
    uint subgroupIdx = gl_LocalInvocationID.y; // 0..3
    uint subTileRow = subgroupIdx >> 1;        // 0 or 1
    uint subTileCol = subgroupIdx & 1u;        // 0 or 1
    uint subTileRow0 = tileRow0 + subTileRow * SUB_TILE;
    uint subTileCol0 = tileCol0 + subTileCol * SUB_TILE;

    // Per-subgroup accumulator (16x16)
    coopmat<float, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseAccumulator> accumulator =
        coopmat<float, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseAccumulator>(0.0);

    // Loop K in 32-wide tiles; each workgroup loads A(32x32) and B(32x32) into shared
    for (uint kTile0 = 0; kTile0 < K; kTile0 += TILE) {
        // Coop load of 32x32 tiles into shared memory (all 128 threads contribute)
        uint linearThread = gl_LocalInvocationID.y * 32u + gl_LocalInvocationID.x; // 0..127
        for (uint idx = linearThread; idx < TILE * TILE; idx += 128u) {
            uint r = idx / TILE; // 0..31
            uint c = idx % TILE; // 0..31

            // Global indices for A and B
            uint aRow = tileRow0 + r;
            uint aCol = kTile0 + c;
            uint bRow = kTile0 + r;
            uint bCol = tileCol0 + c;

            float16_t aVal16 = float16_t(0.0);
            float16_t bVal16 = float16_t(0.0);

            // Bounds-safe loads; zero-pad outside
            if (aRow < M && aCol < K) {
                uint aIndex;
                if (TRANSPOSE_A == 0u) {
                    // A as MxK row-major
                    aIndex = aBase + aRow * K + aCol;
                } else {
                    // A stored transposed as KxM row-major
                    aIndex = aBase + aCol * M + aRow;
                }
                aVal16 = float16_t(float_buffer[aBufferRID].at[aIndex]);
            }
            if (bRow < K && bCol < N) {
                uint bIndex = bBase + bRow * N + bCol; // B as KxN row-major
                bVal16 = float16_t(float_buffer[bBufferRID].at[bIndex]);
            }

            Asub[idx] = aVal16;
            Bsub[idx] = bVal16;
        }

        // Ensure tiles are fully populated before compute
        barrier();

        // Process this 32-wide K block as two 16-wide steps
        for (uint kInner = 0u; kInner < TILE; kInner += SUB_TILE) {
            // Load 16x16 fragments from shared memory for this subgroup's subtile
            coopmat<float16_t, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseA> tile_A;
            coopmat<float16_t, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseB> tile_B;

            // A subtile starts at (subTileRow*16, kInner)
            uint aSharedBase = (subTileRow * SUB_TILE) * TILE + kInner;
            // B subtile starts at (kInner, subTileCol*16)
            uint bSharedBase = kInner * TILE + (subTileCol * SUB_TILE);

            coopMatLoad(tile_A, Asub, aSharedBase, TILE, 0);
            coopMatLoad(tile_B, Bsub, bSharedBase, TILE, 0);

            // FMA accumulate 16x16
            accumulator = coopMatMulAdd(tile_A, tile_B, accumulator);
        }

        // Wait before overwriting shared tiles in next K block
        barrier();
    }

    // Store the resulting 16x16 subtile into C (assumes tiles fit or host guards)
    uint cElement = cBase + subTileRow0 * N + subTileCol0;
    uint cStride = N;
    if (SUM_C == 0u) {
        coopMatStore(accumulator, float_buffer[cBufferRID].at, cElement, cStride, 0);
    } else {
        coopmat<float, gl_ScopeSubgroup, SUB_TILE, SUB_TILE, gl_MatrixUseAccumulator> cTile;
        coopMatLoad(cTile, float_buffer[cBufferRID].at, cElement, cStride, 0);
        cTile = cTile + accumulator;
        coopMatStore(cTile, float_buffer[cBufferRID].at, cElement, cStride, 0);
    }
}
#endif